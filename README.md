# assignment-spark-kafka
spark and kafka apps

####How you feel the field of data engineering is going to evolve over the next 3 years
```
As the data volumes grew and interest in data analytics increased, in the past ten years, new technologies were invented. Some of them died, and others became widely adopted. As modern BI tools allowed analysts and business people to create dashboards, data engineering became a new discipline, applying software engineering principles to ETL development using a new set of tools.

Data engineering, which involves collecting, provisioning and maintaining excellent quality data for insights, is essential in driving business outcomes. For this, a data engineer needs to design and develop a scalable data architecture, set up processes that pool data from multiple sources, check the data quality, and eliminate corrupt data, etc. 

Creating a data pipeline may sound easy, but at big data scale, this meant bringing together a dozen different technologies. A data engineer had to understand a myriad of technologies in-depth, pick the right tool for the job and write code in Scala, Java or Python to create resilient and scalable solutions. A data engineer had to know their data to be able to create jobs which benefit from the power of distributed processing. A data engineer had to understand the infrastructure to be able to identify reasons for failed jobs.

I do not think that technologies like Apache Spark will become any less popular in the next few years as they are great for complex data transformations. Still, the high rate of adoption of cloud data warehouses such as Redshift, Snowflake and Google BigQuery indicates that there are certain advantages they provide. One of them is that Spark requires highly specialised skills, whereas ETL solutions on top of cloud data platforms are heavily reliant on SQL skills even for big data â€” such roles are much easier to fill.

New tools allow data engineers to focus on core data infrastructure, performance optimisation, custom data ingestion pipelines and overall pipeline orchestration. At the same time, data transformation code in those pipelines can be owned by anyone who is comfortable with SQL. For example, analytics engineering is starting to become a thing. This role sits at the intersection of data engineering and data analytics and focuses on data transformation and data quality. Cloud data warehouse engineering is another one.

The "data gap" between data producers and consumers will shrink. As 

Below are the area's where we will see the major development.
*****************
Data Mesh
Data Pipeline Orchestraction
Data Quality
Secure Data Transfers
Data orchestration & Data Lakes & Data Security & Governanace
Cloud Adoptions
Data Caching orchestration
Compute Orchestraction
*****************

```
